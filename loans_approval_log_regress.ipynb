{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3d6847a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" IMPORTS \"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from scipy import stats\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from matplotlib import pyplot\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bab4f09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_excel(\"./datasetTest_2features.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807a10b0",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93787c4",
   "metadata": {},
   "source": [
    "DATA CLEANING   -   FUNCTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9ae009",
   "metadata": {},
   "source": [
    "Removing outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d9309c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    For each column, it first computes the Z-score of each value in the column, relative to the column mean and standard deviation.\n",
    "    It then takes the absolute Z-score because the direction does not matter, only if it is below the threshold.\n",
    "    all(axis=1) ensures that for each row, all column satisfy the constraint.\n",
    "    Finally, the result of this condition is used to index the dataframe\n",
    "\"\"\"\n",
    "def remove_outliers_v1(df):\n",
    "    return df[(np.abs(stats.zscore(df)) < 3).all(axis=1)]\n",
    "\n",
    "def remove_outliers_v2(df):\n",
    "    for column in df:\n",
    "        if(column == 0):\n",
    "            pass\n",
    "        else:\n",
    "            q_low = df[column].quantile(0.05)\n",
    "            q_high  = df[column].quantile(0.95)\n",
    "            df_ = df[(df[column] > q_low) & (df[column] < q_high)]\n",
    "    return df_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da692ca",
   "metadata": {},
   "source": [
    "Removing duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc59fb7",
   "metadata": {},
   "source": [
    "Fill missing values with imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e3f02126",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imputing_miss_values(df):\n",
    "    # print total missing\n",
    "    print('Missing: %d' % sum(np.isnan(df.values).flatten()))\n",
    "    # define imputer\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    # fit on the dataset\n",
    "    imputer.fit(df)\n",
    "    # transform the dataset\n",
    "    df_ = imputer.transform(df)\n",
    "    # print total missing\n",
    "    print('Missing after imputation: %d' % sum(np.isnan(df_).flatten()))\n",
    "    df_ = pd.DataFrame(df_)\n",
    "    return df_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f4671b",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9e32d9",
   "metadata": {},
   "source": [
    "NORMALIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6e616f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using The maximum absolute scaling\n",
    "# The maximum absolute scaling rescales each feature between -1 and 1\n",
    "def max_scaling(X):\n",
    "    X_max_scaled = X.copy()\n",
    "    for column in X:\n",
    "        X_max_scaled[column] = X_max_scaled[column] / X_max_scaled[column].abs().max()\n",
    "    return X_max_scaled\n",
    "\n",
    "# Using The min-max feature scaling\n",
    "# The min-max approach (often called normalization) rescales the feature to a hard and fast range of [0,1]\n",
    "def min_max_scaling(X):\n",
    "    X_min_max_scaled = X.copy()\n",
    "    for column in X:\n",
    "        X_min_max_scaled[column] = (X_min_max_scaled[column] - X_min_max_scaled[column].min()) / (X_min_max_scaled[column].max() - X_min_max_scaled[column].min())\n",
    "    return X_min_max_scaled\n",
    "\n",
    "\n",
    "# Using The z-score method\n",
    "# The z-score method (often called standardization) transforms the info into distribution with a mean of 0 and a typical deviation of 1\n",
    "def z_scaling(X):\n",
    "    X_z_scaled = X.copy()\n",
    "    for column in X:\n",
    "        X_z_scaled[column] = (X_z_scaled[column] - X_z_scaled[column].mean()) / X_z_scaled[column].std()\n",
    "    return X_z_scaled\n",
    "\n",
    "def get_train_mean(X_train):\n",
    "    train_mean_list = []\n",
    "    for column in X_train:\n",
    "        train_mean_list.append(X_train[column].mean())\n",
    "    return train_mean_list\n",
    "\n",
    "def get_train_std(X_train):\n",
    "    train_std_list = []\n",
    "    for column in X_train:\n",
    "        train_std_list.append(X_train[column].std())\n",
    "    return train_std_list\n",
    "\n",
    "def scaling(X_train, X_test, scaler): # Standard or MinMax\n",
    "    # Get scaling parameters with the train sample exclusively, using the Scaler.fit() function\n",
    "    scaler.fit(X_train)\n",
    "    # Scale data using Scaler.transform()\n",
    "    X_train_scaled = pd.DataFrame(scaler.transform(X_train))\n",
    "    X_test_scaled = pd.DataFrame(scaler.transform(X_test))\n",
    "    return X_train_scaled, X_test_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73e1f63",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a43ecf4",
   "metadata": {},
   "source": [
    "DATA PREPARATION - APPLICATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "23dcfc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dataset.copy()\n",
    "column = df['action_taken'].replace(3, 0)\n",
    "df['action_taken'] = column.values\n",
    "\n",
    "# features to consider removing\n",
    "considered_features = []\n",
    "for column in df:\n",
    "    considered_features.append(column)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbf792d",
   "metadata": {},
   "source": [
    "DATA CLEANING - APPLICATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ef8f31f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Removing outliers \"\"\"\n",
    "df_no_outliers = remove_outliers_v2(df)\n",
    "\n",
    "\"\"\" Removing duplicates \"\"\"\n",
    "df_no_outliers_and_duplicates = df_no_outliers.drop_duplicates()\n",
    "\n",
    "\"\"\" Counting nan \"\"\"\n",
    "df_no_outliers_and_duplicates['action_taken'].isnull().sum(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "eab3f89a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing: 19092\n",
      "Missing after imputation: 0\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Imputing missing values \"\"\"\n",
    "df_ = imputing_miss_values(df_no_outliers_and_duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9f162fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Spliting dataset \"\"\"\n",
    "# define dataset\n",
    "X, y = df_.drop(columns=0), df_[[0]]\n",
    "# split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "af40ee73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.datatrigger.org/post/scaling/\n",
    "\n",
    "\"\"\" Normalizing data \"\"\"\n",
    "# Instantiate a Scaler --> Standard or MinMaxScaler()\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled, X_test_scaled = scaling(X_train, X_test, scaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89144ec1",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b2a628",
   "metadata": {},
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84d6f4e",
   "metadata": {},
   "source": [
    "https://machinelearningmastery.com/implement-logistic-regression-stochastic-gradient-descent-scratch-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1a3f60",
   "metadata": {},
   "source": [
    "Making Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "2fff98d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "to_add = y_train.values\n",
    "dataset = X_train_scaled.copy()\n",
    "dataset['action_taken'] = to_add\n",
    "dataset = dataset.values.tolist()\n",
    "\n",
    "mu, sigma = 0, 1 # mean and standard deviation\n",
    "s = np.random.normal(mu, sigma, 13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "0163d4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Logistic Regression from scratch \"\"\"\n",
    "\n",
    "# Make a prediction with coefficients\n",
    "def predict(row, coefficients):\n",
    "\tyhat = coefficients[0]\n",
    "\tfor i in range(len(row)-1):\n",
    "\t\tyhat += coefficients[i + 1] * row[i]\n",
    "\treturn 1.0 / (1.0 + math.exp(-yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "cf997036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good pred: 129796\n",
      "Bad pred: 62943\n",
      "Good pred = 0.67 %\n"
     ]
    }
   ],
   "source": [
    "coef = s.tolist()\n",
    "good_pred, bad_pred = 0, 0\n",
    "for row in dataset:\n",
    "\tyhat = predict(row, coef)\n",
    "\tif(row[-1] == round(yhat)):\n",
    "\t\tgood_pred += 1\n",
    "\telse:\n",
    "\t\tbad_pred += 1\n",
    "print(\"Good pred: {}\".format(good_pred))\n",
    "print(\"Bad pred: {}\".format(bad_pred))\n",
    "print(\"Good pred = {} %\".format(round(good_pred/(good_pred+bad_pred), 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce84621",
   "metadata": {},
   "source": [
    "Estimating Coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "92daf0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate logistic regression coefficients using stochastic gradient descent\n",
    "def coefficients_sgd(train, l_rate, n_epoch):\n",
    "\tcoef = [0.0 for i in range(len(train[0]))]\n",
    "\tfor epoch in range(n_epoch):\n",
    "\t\tsum_error = 0\n",
    "\t\tfor row in train:\n",
    "\t\t\tyhat = predict(row, coef)\n",
    "\t\t\terror = row[-1] - yhat\n",
    "\t\t\tsum_error += error**2\n",
    "\t\t\tcoef[0] = coef[0] + l_rate * error * yhat * (1.0 - yhat)\n",
    "\t\t\tfor i in range(len(row)-1):\n",
    "\t\t\t\tcoef[i + 1] = coef[i + 1] + l_rate * error * yhat * (1.0 - yhat) * row[i]\n",
    "\t\tprint('>epoch=%d, lrate=%.3f, error=%.3f' % (epoch, l_rate, sum_error))\n",
    "\treturn coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "b5e6255b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">epoch=0, lrate=0.300, error=24193.779\n",
      ">epoch=1, lrate=0.300, error=24080.785\n",
      ">epoch=2, lrate=0.300, error=24090.939\n",
      ">epoch=3, lrate=0.300, error=24093.168\n",
      ">epoch=4, lrate=0.300, error=24090.192\n",
      ">epoch=5, lrate=0.300, error=24129.493\n",
      ">epoch=6, lrate=0.300, error=24091.614\n",
      ">epoch=7, lrate=0.300, error=24138.705\n",
      ">epoch=8, lrate=0.300, error=24086.164\n",
      ">epoch=9, lrate=0.300, error=24139.839\n",
      ">epoch=10, lrate=0.300, error=24088.051\n",
      ">epoch=11, lrate=0.300, error=24090.763\n",
      ">epoch=12, lrate=0.300, error=24089.714\n",
      ">epoch=13, lrate=0.300, error=24090.680\n",
      ">epoch=14, lrate=0.300, error=24090.663\n",
      ">epoch=15, lrate=0.300, error=24102.695\n",
      ">epoch=16, lrate=0.300, error=24088.665\n",
      ">epoch=17, lrate=0.300, error=24090.320\n",
      ">epoch=18, lrate=0.300, error=24095.957\n",
      ">epoch=19, lrate=0.300, error=24089.305\n",
      ">epoch=20, lrate=0.300, error=24087.304\n",
      ">epoch=21, lrate=0.300, error=24087.945\n",
      ">epoch=22, lrate=0.300, error=24106.731\n",
      ">epoch=23, lrate=0.300, error=24102.185\n",
      ">epoch=24, lrate=0.300, error=24089.820\n",
      ">epoch=25, lrate=0.300, error=24090.596\n",
      ">epoch=26, lrate=0.300, error=24107.287\n",
      ">epoch=27, lrate=0.300, error=24095.507\n",
      ">epoch=28, lrate=0.300, error=24094.375\n",
      ">epoch=29, lrate=0.300, error=24110.616\n",
      ">epoch=30, lrate=0.300, error=24087.663\n",
      ">epoch=31, lrate=0.300, error=24135.977\n",
      ">epoch=32, lrate=0.300, error=24094.975\n",
      ">epoch=33, lrate=0.300, error=24098.193\n",
      ">epoch=34, lrate=0.300, error=24098.317\n",
      ">epoch=35, lrate=0.300, error=24090.291\n",
      ">epoch=36, lrate=0.300, error=24090.288\n",
      ">epoch=37, lrate=0.300, error=24094.848\n",
      ">epoch=38, lrate=0.300, error=24118.217\n",
      ">epoch=39, lrate=0.300, error=24089.631\n",
      ">epoch=40, lrate=0.300, error=24090.307\n",
      ">epoch=41, lrate=0.300, error=24090.308\n",
      ">epoch=42, lrate=0.300, error=24085.336\n",
      ">epoch=43, lrate=0.300, error=24085.346\n",
      ">epoch=44, lrate=0.300, error=24103.169\n",
      ">epoch=45, lrate=0.300, error=24087.231\n",
      ">epoch=46, lrate=0.300, error=24093.102\n",
      ">epoch=47, lrate=0.300, error=24135.105\n",
      ">epoch=48, lrate=0.300, error=24088.259\n",
      ">epoch=49, lrate=0.300, error=24110.957\n",
      ">epoch=50, lrate=0.300, error=24095.652\n",
      ">epoch=51, lrate=0.300, error=24085.962\n",
      ">epoch=52, lrate=0.300, error=24107.079\n",
      ">epoch=53, lrate=0.300, error=24111.684\n",
      ">epoch=54, lrate=0.300, error=24110.111\n",
      ">epoch=55, lrate=0.300, error=24099.071\n",
      ">epoch=56, lrate=0.300, error=24142.951\n",
      ">epoch=57, lrate=0.300, error=24089.141\n",
      ">epoch=58, lrate=0.300, error=24088.945\n",
      ">epoch=59, lrate=0.300, error=24097.795\n",
      ">epoch=60, lrate=0.300, error=24134.821\n",
      ">epoch=61, lrate=0.300, error=24132.872\n",
      ">epoch=62, lrate=0.300, error=24088.011\n",
      ">epoch=63, lrate=0.300, error=24111.619\n",
      ">epoch=64, lrate=0.300, error=24101.754\n",
      ">epoch=65, lrate=0.300, error=24108.858\n",
      ">epoch=66, lrate=0.300, error=24099.846\n",
      ">epoch=67, lrate=0.300, error=24098.537\n",
      ">epoch=68, lrate=0.300, error=24101.608\n",
      ">epoch=69, lrate=0.300, error=24101.382\n",
      ">epoch=70, lrate=0.300, error=24101.648\n",
      ">epoch=71, lrate=0.300, error=24102.120\n",
      ">epoch=72, lrate=0.300, error=24095.219\n",
      ">epoch=73, lrate=0.300, error=24108.378\n",
      ">epoch=74, lrate=0.300, error=24105.322\n",
      ">epoch=75, lrate=0.300, error=24108.491\n",
      ">epoch=76, lrate=0.300, error=24112.850\n",
      ">epoch=77, lrate=0.300, error=24098.562\n",
      ">epoch=78, lrate=0.300, error=24111.899\n",
      ">epoch=79, lrate=0.300, error=24090.549\n",
      ">epoch=80, lrate=0.300, error=24139.428\n",
      ">epoch=81, lrate=0.300, error=24113.382\n",
      ">epoch=82, lrate=0.300, error=24090.472\n",
      ">epoch=83, lrate=0.300, error=24108.445\n",
      ">epoch=84, lrate=0.300, error=24089.693\n",
      ">epoch=85, lrate=0.300, error=24113.396\n",
      ">epoch=86, lrate=0.300, error=24107.191\n",
      ">epoch=87, lrate=0.300, error=24118.498\n",
      ">epoch=88, lrate=0.300, error=24118.480\n",
      ">epoch=89, lrate=0.300, error=24129.109\n",
      ">epoch=90, lrate=0.300, error=24098.108\n",
      ">epoch=91, lrate=0.300, error=24093.792\n",
      ">epoch=92, lrate=0.300, error=24145.338\n",
      ">epoch=93, lrate=0.300, error=24114.130\n",
      ">epoch=94, lrate=0.300, error=24091.415\n",
      ">epoch=95, lrate=0.300, error=24091.163\n",
      ">epoch=96, lrate=0.300, error=24091.161\n",
      ">epoch=97, lrate=0.300, error=24136.549\n",
      ">epoch=98, lrate=0.300, error=24122.450\n",
      ">epoch=99, lrate=0.300, error=24107.833\n",
      "[12.187267878525955, -0.23398196484645772, 0.6351714392427639, -0.16191833598138133, 0.0399222563748483, -0.0013612024843263695, -0.08813482800920544, -0.23285517716701645, -0.20719766336743844, -1.3092183858950235, 17.97740885233127, 0.01252401796104878, -0.046135546367722616]\n"
     ]
    }
   ],
   "source": [
    "# Calculate coefficients\n",
    "l_rate = 0.3\n",
    "n_epoch = 100\n",
    "coef = coefficients_sgd(dataset, l_rate, n_epoch)\n",
    "print(coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "f7a8f269",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randrange\n",
    "# Find the min and max values for each column\n",
    "def dataset_minmax(dataset):\n",
    "\tminmax = list()\n",
    "\tfor i in range(len(dataset[0])):\n",
    "\t\tcol_values = [row[i] for row in dataset]\n",
    "\t\tvalue_min = min(col_values)\n",
    "\t\tvalue_max = max(col_values)\n",
    "\t\tminmax.append([value_min, value_max])\n",
    "\treturn minmax\n",
    "\n",
    "# Rescale dataset columns to the range 0-1\n",
    "def normalize_dataset(dataset, minmax):\n",
    "\tfor row in dataset:\n",
    "\t\tfor i in range(len(row)):\n",
    "\t\t\trow[i] = (row[i] - minmax[i][0]) / (minmax[i][1] - minmax[i][0])\n",
    "\n",
    "# Split a dataset into k folds\n",
    "def cross_validation_split(dataset, n_folds):\n",
    "\tdataset_split = list()\n",
    "\tdataset_copy = list(dataset)\n",
    "\tfold_size = int(len(dataset) / n_folds)\n",
    "\tfor i in range(n_folds):\n",
    "\t\tfold = list()\n",
    "\t\twhile len(fold) < fold_size:\n",
    "\t\t\tindex = randrange(len(dataset_copy))\n",
    "\t\t\tfold.append(dataset_copy.pop(index))\n",
    "\t\tdataset_split.append(fold)\n",
    "\treturn dataset_split\n",
    "\n",
    "# Calculate accuracy percentage\n",
    "def accuracy_metric(actual, predicted):\n",
    "\tcorrect = 0\n",
    "\tfor i in range(len(actual)):\n",
    "\t\tif actual[i] == predicted[i]:\n",
    "\t\t\tcorrect += 1\n",
    "\treturn correct / float(len(actual)) * 100.0\n",
    "\n",
    "# Evaluate an algorithm using a cross validation split\n",
    "def evaluate_algorithm(dataset, algorithm, n_folds, *args):\n",
    "\tfolds = cross_validation_split(dataset, n_folds)\n",
    "\tscores = list()\n",
    "\tfor fold in folds:\n",
    "\t\ttrain_set = list(folds)\n",
    "\t\ttrain_set.remove(fold)\n",
    "\t\ttrain_set = sum(train_set, [])\n",
    "\t\ttest_set = list()\n",
    "\t\tfor row in fold:\n",
    "\t\t\trow_copy = list(row)\n",
    "\t\t\ttest_set.append(row_copy)\n",
    "\t\t\trow_copy[-1] = None\n",
    "\t\tpredicted = algorithm(train_set, test_set, *args)\n",
    "\t\tactual = [row[-1] for row in fold]\n",
    "\t\taccuracy = accuracy_metric(actual, predicted)\n",
    "\t\tscores.append(accuracy)\n",
    "\treturn scores\n",
    "\n",
    "# Make a prediction with coefficients\n",
    "def predict(row, coefficients):\n",
    "\tyhat = coefficients[0]\n",
    "\tfor i in range(len(row)-1):\n",
    "\t\tyhat += coefficients[i + 1] * row[i]\n",
    "\treturn 1.0 / (1.0 + math.exp(-yhat))\n",
    " \n",
    "# Estimate logistic regression coefficients using stochastic gradient descent\n",
    "def coefficients_sgd(train, l_rate, n_epoch):\n",
    "\tcoef = [0.0 for i in range(len(train[0]))]\n",
    "\tfor epoch in range(n_epoch):\n",
    "\t\tfor row in train:\n",
    "\t\t\tyhat = predict(row, coef)\n",
    "\t\t\terror = row[-1] - yhat\n",
    "\t\t\tcoef[0] = coef[0] + l_rate * error * yhat * (1.0 - yhat)\n",
    "\t\t\tfor i in range(len(row)-1):\n",
    "\t\t\t\tcoef[i + 1] = coef[i + 1] + l_rate * error * yhat * (1.0 - yhat) * row[i]\n",
    "\treturn coef\n",
    " \n",
    "# Linear Regression Algorithm With Stochastic Gradient Descent\n",
    "def logistic_regression(train, test, l_rate, n_epoch):\n",
    "\tpredictions = list()\n",
    "\tcoef = coefficients_sgd(train, l_rate, n_epoch)\n",
    "\tfor row in test:\n",
    "\t\tyhat = predict(row, coef)\n",
    "\t\tyhat = round(yhat)\n",
    "\t\tpredictions.append(yhat)\n",
    "\treturn(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "a2e507b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [80.46540586816094, 81.08542817858718, 81.18400913170935, 80.96090486938023, 80.59252341297636]\n",
      "Mean Accuracy: 80.858%\n"
     ]
    }
   ],
   "source": [
    "# normalize\n",
    "minmax = dataset_minmax(dataset)\n",
    "normalize_dataset(dataset, minmax)\n",
    "# evaluate algorithm\n",
    "n_folds = 5\n",
    "l_rate = 0.1\n",
    "n_epoch = 100\n",
    "scores = evaluate_algorithm(dataset, logistic_regression, n_folds, l_rate, n_epoch)\n",
    "print('Scores: %s' % scores)\n",
    "print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138cb344",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4596a356",
   "metadata": {},
   "source": [
    "2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "e2cf585a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 81.26\n"
     ]
    }
   ],
   "source": [
    "# fit the model\n",
    "model = LogisticRegression(solver='liblinear')\n",
    "model.fit(X_train_scaled, y_train.values.ravel())   #.values will give the values in a numpy array (shape: (n,1)) \n",
    "                                                    #.ravel will convert that array shape to (n, ) (i.e. flatten it)\n",
    "# evaluate the model\n",
    "yhat = model.predict(X_test_scaled)\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(y_test, yhat)\n",
    "print('Accuracy: %.2f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c6af76",
   "metadata": {},
   "source": [
    "3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "143c17cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">0.0000 0.813 (0.003)\n",
      ">0.0001 0.763 (0.001)\n",
      ">0.0010 0.806 (0.003)\n",
      ">0.0100 0.815 (0.003)\n",
      ">0.1000 0.814 (0.003)\n",
      ">1.0000 0.813 (0.003)\n"
     ]
    }
   ],
   "source": [
    "# get a list of models to evaluate\n",
    "def get_models():\n",
    "\tmodels = dict()\n",
    "\tfor p in [0.0, 0.0001, 0.001, 0.01, 0.1, 1.0]:\n",
    "\t\t# create name for model\n",
    "\t\tkey = '%.4f' % p\n",
    "\t\t# turn off penalty in some cases\n",
    "\t\tif p == 0.0:\n",
    "\t\t\t# no penalty in this case\n",
    "\t\t\tmodels[key] = LogisticRegression(solver='lbfgs', penalty='none')\n",
    "\t\telse:\n",
    "\t\t\tmodels[key] = LogisticRegression(solver='lbfgs', penalty='l2', C=p)\n",
    "\treturn models\n",
    "\n",
    "# evaluate a give model using cross-validation\n",
    "def evaluate_model(model, X, y):\n",
    "\t# define the evaluation procedure\n",
    "\tcv = RepeatedStratifiedKFold(n_splits=12, n_repeats=3, random_state=1)\n",
    "\t# evaluate the model\n",
    "\tscores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "\treturn scores\n",
    "\n",
    "# get the models to evaluate\n",
    "models = get_models()\n",
    "# evaluate the models and store results\n",
    "results, names = list(), list()\n",
    "for name, model in models.items():\n",
    "\t# evaluate the model and collect the scores\n",
    "\tscores = evaluate_model(model, X_train_scaled, y_train)\n",
    "\t# store the results\n",
    "\tresults.append(scores)\n",
    "\tnames.append(name)\n",
    "\t# summarize progress along the way\n",
    "\tprint('>%s %.3f (%.3f)' % (name, np.mean(scores), np.std(scores)))\n",
    "# plot model performance for comparison\n",
    "# pyplot.boxplot(results, labels=names, showmeans=True)\n",
    "# pyplot.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "81794d4967e6c3204c66dcd87b604927b115b27c00565d3d43f05ba2f3a2cb0d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
